---
title: Simulation
description: Test your robots in virtual environments with sim2d and sim3d
order: 25
---

# Simulation

Test and validate your robot algorithms in virtual environments before deploying to real hardware. HORUS provides two simulation tools optimized for different use cases.

---

## Overview

HORUS provides **two levels of simulation**:

| Level | Tools | Use Case |
|-------|-------|----------|
| **Environment Simulation** | sim2d, sim3d | Full robot + world simulation |
| **Node Simulation Mode** | Built into nodes | Hardware prototyping without devices |

### Environment Simulators

| Simulator | Status | Dimensions | Use Case | Performance |
|-----------|--------|------------|----------|-------------|
| **sim2d** | Active Development | 2D Top-Down | Fast prototyping, navigation | 1000+ Hz headless |
| **sim3d** | Available | Full 3D | Realistic testing, RL training | 60 FPS visual, 100K+ steps/sec RL |

### Node Simulation Mode

Every hardware node (IMU, GPS, LiDAR, Camera, Serial, etc.) has a built-in `simulation` mode that generates synthetic data. See [Node Simulation Mode](#node-simulation-mode) below.

**Key Benefits:**
- **Same code works in sim and reality** - Write once, deploy anywhere
- **HORUS-native** - Direct Hub integration, no middleware
- **Sub-microsecond IPC** - Realistic latency testing
- **CI/CD ready** - Headless mode for automated testing
- **No hardware required** - Develop before hardware arrives

---

## sim2d - 2D Robotics Simulator

> **Status:** Active Development - Core features working, polish ongoing

### What is sim2d?

A lightweight **2D top-down simulator** built with Bevy and Rapier2D for fast iteration and testing.

**Perfect for:**
- Navigation algorithm development
- Multi-robot coordination
- Path planning testing
- Sensor simulation (LiDAR, odometry, IMU)
- CI/CD automated testing

**Not designed for:**
- Realistic 3D visualization
- Camera/vision simulation
- Complex robot modeling

### Quick Start

#### Installation

```bash
# Ubuntu/Debian - Install dependencies
sudo apt install -y pkg-config libx11-dev libasound2-dev libudev-dev

# Set environment variables
export PKG_CONFIG_ALLOW_SYSTEM_LIBS=1
export PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1
```

#### Run sim2d

```bash
# Launch simulator
horus sim2d

# Or with specific options
horus sim2d --world warehouse.yaml
horus sim2d --headless  # No GUI (for testing)
```

### Features

#### Visual Mode
- Real-time 2D rendering with Bevy
- Top-down view of robots and environment
- Debug visualization (lidar rays, paths, collision shapes)
- Interactive camera controls

#### Physics
- Rapier2D physics engine
- Realistic differential drive kinematics
- Collision detection and response
- Configurable physics rate (default: 240 Hz)

#### Sensors
- **2D LiDAR**: 360° scanning with configurable resolution
- **Odometry**: Wheel encoder simulation with realistic noise
- **IMU**: Angular velocity and acceleration
- **Ground truth**: Perfect pose for debugging

#### HORUS Integration

Your robot code works **unchanged** in simulation:

```rust
use horus::prelude::*;
use horus::prelude::*; // Provides CmdVel;

struct NavigationNode {
    cmd_pub: Hub<CmdVel>,
    scan_sub: Hub<LaserScan>,
}

impl NavigationNode {
    fn new() -> Result<Self> {
        Ok(Self {
            cmd_pub: Hub::new("cmd_vel")?,
            scan_sub: Hub::new("scan")?,
        })
    }
}

impl Node for NavigationNode {
    fn name(&self) -> &'static str {
        "NavigationNode"
    }

    fn tick(&mut self, mut ctx: Option<&mut NodeInfo>) {
        // Read simulated LiDAR
        if let Some(scan) = self.scan_sub.recv(&mut ctx) {
            // Compute motion command
            let cmd = avoid_obstacles(&scan);

            // Send to simulated robot
            self.cmd_pub.send(cmd, &mut ctx).ok();
        }
    }
}
```

**Same topics work in simulation and reality:**
- `/cmd_vel` - Velocity commands
- `/scan` - LiDAR data
- `/odom` - Odometry
- `/tf` - Transform frames

### Usage Examples

#### Test Navigation Algorithm

```bash
# Terminal 1: Launch simulator with maze
horus sim2d --world maze.yaml

# Terminal 2: Run your navigation code
horus run navigation.rs --release

# Terminal 3: Monitor performance
horus monitor
```

#### Headless Testing (CI/CD)

```bash
#!/bin/bash
# Automated test script

# Start simulator in headless mode
horus sim2d --headless &
SIM_PID=$!

# Wait for simulator to initialize
sleep 2

# Run test
timeout 30 horus run test_navigation.rs --release
TEST_RESULT=$?

# Stop simulator
kill $SIM_PID

exit $TEST_RESULT
```

#### Custom Scenarios

Create a scenario configuration file:

```yaml
# warehouse.yaml
world:
  size: [20.0, 20.0]

obstacles:
  - type: box
    position: [5.0, 5.0]
    size: [2.0, 1.0]

  - type: circle
    position: [10.0, 8.0]
    radius: 1.5

robots:
  - name: robot1
    position: [1.0, 1.0]
    orientation: 0.0
```

Load it:
```bash
horus sim2d --world warehouse.yaml
```

### Configuration

sim2d can be configured via `sim2d_config.yaml`:

```yaml
physics:
  rate_hz: 240
  gravity: [0.0, 0.0]

rendering:
  window_size: [1280, 720]
  target_fps: 60

robot:
  max_linear_vel: 0.5  # m/s
  max_angular_vel: 2.0 # rad/s
  wheel_radius: 0.05   # meters
  wheel_separation: 0.2

sensors:
  lidar:
    num_rays: 360
    max_range: 10.0
    fov: 6.28318  # 2π (360°)
    rate_hz: 10
    noise_std: 0.01
```

### Command Reference

```bash
horus sim2d [OPTIONS]

Options:
  --headless                 Run without GUI (for CI/CD)
  --world <FILE>             World configuration file
  --world-image <FILE>       World image file (PNG, JPG, PGM)
  --resolution <FLOAT>       Resolution in meters per pixel
  --threshold <0-255>        Obstacle threshold (darker = obstacle)
  --robot <FILE>             Robot configuration file
  --topic <PREFIX>           HORUS topic prefix (default: /robot)
  --name <NAME>              Robot name for logging (default: robot)
  -h, --help                 Show help
```

### Performance

**Visual Mode:**
- Rendering: 60 FPS
- Physics: 240 Hz
- IPC latency: 85-167ns

**Headless Mode:**
- Physics: 1000+ Hz
- IPC latency: &lt;100ns
- Memory: ~50MB

### Known Limitations

As sim2d is under active development, be aware of:

- Limited to 2D top-down view
- No camera/vision sensors yet
- Basic visualization (no shadows, advanced graphics)
- Documentation being updated

For production use, we recommend:
- Testing critical algorithms in sim2d first
- Validating on real hardware before deployment

---

## sim3d - 3D Robotics Simulator with RL Support

> **Status:** Available - Core features implemented, advanced features in development

### Overview

A **production-grade 3D simulator** built with Bevy and Rapier3D, designed as the evolution of sim2d with first-class reinforcement learning support.

### Features

#### Dual-Mode Operation

**Visual Mode (60 FPS):**
- Realistic 3D rendering with PBR materials
- Dynamic shadows and lighting
- Multiple camera views (orbital, follow, first-person)
- Debug visualization (TF frames, collision shapes, sensor FOVs)
- Interactive scene editor

**Headless Mode (100K+ steps/sec):**
- No rendering overhead
- Vectorized environments (1024+ parallel instances)
- Fast reset (&lt;1ms)
- Optimized for RL training

#### Robot Support

**URDF Loading:**
- Load standard robot descriptions
- Automatic TF tree generation
- Mesh loading (GLTF, STL, OBJ)
- Material and texture support
- Joint controller generation

**Example robots:**
- TurtleBot3 (differential drive)
- UR5e (6-DOF manipulator)
- Quadrotors (flight control)
- Custom robots via URDF

#### Advanced Physics

Built on **Rapier3D** with:
- Rigid body dynamics
- Articulated robots (multi-DOF joints)
- Collision detection with continuous collision detection (CCD)
- Constraint solvers (revolute, prismatic, fixed)
- Contact forces and friction
- 240 Hz physics rate

#### 3D Sensors

**LiDAR 3D:**
- 360° horizontal × 30° vertical scanning
- Configurable resolution (e.g., 720×16 rays)
- Realistic noise models
- Point cloud generation (&lt;5ms)

**RGB-D Camera:**
- RGB images
- Depth maps
- Configurable resolution and FOV
- Lens distortion simulation

**IMU:**
- Linear acceleration
- Angular velocity
- Realistic drift and noise

**GPS:**
- Global positioning with noise
- Configurable accuracy

**Force/Torque Sensors:**
- Contact force measurement
- Joint torque feedback

#### Transform Frames (TF)

**ROS-compatible TF system:**
- Hierarchical frame tree
- Automatic TF publishing to `/tf` topic
- Transform lookups between any frames
- Visualization of frame axes
- URDF-based frame hierarchy

```rust
// Get transform between frames
let transform = tf_tree.get_transform("base_link", "camera_link")?;

// Visualize TF tree
horus sim3d --tf-view
```

#### Reinforcement Learning

**Gymnasium Interface:**
```python
import gymnasium as gym
import horus_sim3d

# Create vectorized environment (1024 parallel instances)
env = gym.make_vec("HorusNav3D-v0", num_envs=1024)

obs, info = env.reset()

for step in range(1_000_000):
    actions = policy(obs)
    obs, rewards, dones, truncated, info = env.step(actions)

    # 100K+ steps per second!
```

**Domain Randomization:**
- Physics parameters (mass, friction, damping)
- Visual appearance (colors, textures, lighting)
- Sensor noise (gaussian, dropout, latency)
- Environment layout (randomized obstacles)

**Built-in Tasks:**
- Navigation (point-to-point)
- Obstacle avoidance
- Object manipulation (pick & place)
- Racing (time trial, multi-agent)

#### Scene Management

**Gazebo SDF Import:**
```bash
# Import Gazebo world
horus sim3d --import-sdf warehouse.world
```

**Runtime Spawning:**
```rust
// Spawn objects at runtime
scene.spawn_box(position, size, material);
scene.spawn_urdf("turtlebot3.urdf", pose);
```

**Save/Load Scenes:**
```bash
# Save current scene
horus sim3d --save-scene my_world.yaml

# Load scene
horus sim3d --scene my_world.yaml
```

### Architecture

```
┌─────────────────────────────────────────────────────┐
│                  User Interface                      │
├──────────────┬──────────────┬───────────────────────┤
│ CLI          │ GUI (egui)   │ Python (Gym)          │
│ horus sim3d  │ Debug panels │ RL training           │
└──────┬───────┴──────┬───────┴──────┬────────────────┘
       │              │              │
┌──────▼──────────────▼──────────────▼────────────────┐
│           Simulation Core (Bevy ECS)                │
├─────────────────────────────────────────────────────┤
│                                                     │
│  ┌────────────┐  ┌────────────┐  ┌──────────────┐ │
│  │  Physics   │  │  Sensors   │  │  TF System   │ │
│  │ (Rapier3D) │  │ Simulation │  │  (frames)    │ │
│  └─────┬──────┘  └─────┬──────┘  └──────┬───────┘ │
│        │               │                │         │
│  ┌─────▼───────────────▼────────────────▼───────┐  │
│  │         HORUS Hub Bridge                     │  │
│  │  • Publish sensor data                       │  │
│  │  • Subscribe to cmd_vel                      │  │
│  │  • Publish /tf transforms                    │  │
│  └──────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
         │                            │
┌────────▼─────────┐        ┌─────────▼──────────────┐
│  Visual Rendering│        │  Headless RL Mode      │
│  (60 FPS)        │        │  (100K+ steps/sec)     │
└──────────────────┘        └────────────────────────┘
```

### Technical Stack

**Core:**
- Bevy 0.15 (ECS + rendering)
- Rapier3D 0.22 (physics)
- nalgebra (math)

**Robot Loading:**
- urdf-rs (URDF parsing)
- GLTF/STL/OBJ support

**RL:**
- PyO3 (Python bindings)
- Gymnasium interface
- Rayon (parallel processing)

**Pure Rust:**
- Memory safe
- Cross-platform
- Single binary

### Planned Command Interface

```bash
# Visual mode (default)
horus sim3d

# Headless RL training
horus sim3d --headless --num-envs 1024

# Load URDF robot
horus sim3d --robot turtlebot3.urdf

# Import Gazebo world
horus sim3d --import-sdf warehouse.world

# Enable TF visualization
horus sim3d --tf-view

# Debug mode with editor
horus sim3d --editor

# Specific scenario
horus sim3d --scenario manipulation_task
```

### Python RL Example

```python
# train_navigation.py
import gymnasium as gym
from stable_baselines3 import PPO
import horus_sim3d

# Create environment
env = gym.make_vec("HorusNav3D-v0",
                   num_envs=1024,
                   domain_randomization=True)

# Train agent
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10_000_000)

# Save policy
model.save("navigation_policy")

# Evaluate in visual mode
eval_env = gym.make("HorusNav3D-v0", render_mode="human")
obs, _ = eval_env.reset()

for _ in range(1000):
    action, _ = model.predict(obs)
    obs, reward, done, truncated, info = eval_env.step(action)
    if done or truncated:
        obs, _ = eval_env.reset()
```

### Development Roadmap

**Phase 1: Core Foundation** (Q1 2026)
- [DONE] Technical specification complete
- [WIP] Bevy + Rapier3D integration
- [WIP] Basic robot spawning
- [WIP] Camera controls

**Phase 2: Robot Support** (Q2 2026)
- [WIP] URDF loading
- [WIP] TF system implementation
- [WIP] Articulated robots
- [WIP] Joint controllers

**Phase 3: Sensors** (Q2 2026)
- [WIP] LiDAR 3D
- [WIP] RGB-D camera
- [WIP] IMU, GPS, encoders
- [WIP] Sensor noise models

**Phase 4: RL Features** (Q3 2026)
- [WIP] Headless mode optimization
- [WIP] Vectorized environments
- [WIP] Python Gymnasium bindings
- [WIP] Domain randomization
- [WIP] Built-in tasks

**Phase 5: Polish** (Q3-Q4 2026)
- [WIP] Scene editor
- [WIP] Gazebo SDF import
- [WIP] Documentation
- [WIP] Example projects

### Performance Targets

| Metric | Target |
|--------|--------|
| Visual FPS | 60 FPS |
| Physics rate | 240 Hz |
| Headless step rate | 100,000+ steps/sec (1024 envs) |
| LiDAR generation | &lt;5ms (720×16 rays) |
| URDF loading | &lt;500ms (TurtleBot3) |
| Reset time | &lt;1ms (fast reset) |
| Memory (visual) | &lt;500MB (single robot) |
| Memory (headless) | &lt;50MB per environment |

### Detailed Sim3D Documentation

For in-depth coverage of specific Sim3D features, see:

| Module | Description |
|--------|-------------|
| [Sim3D Editor](/development/sim3d-editor) | Interactive scene editing with gizmos, selection, and property inspection |
| [Sim3D Recording](/development/sim3d-recording) | Trajectory recording, time control, video export, and sensor data bags |
| [Sim3D Multi-Robot](/development/sim3d-multi-robot) | Multi-robot simulation with communication, swarm coordination, and formations |

---

## Node Simulation Mode

Beyond full environment simulators (sim2d/sim3d), HORUS hardware nodes include **built-in simulation mode** for prototyping without physical hardware.

### Why Simulation Mode?

| Use Case | Benefit |
|----------|---------|
| **Early prototyping** | Develop control logic before hardware arrives |
| **CI/CD testing** | Run automated tests without physical devices |
| **Algorithm development** | Test perception/planning pipelines with synthetic data |
| **Team collaboration** | Developers without hardware can still contribute |
| **Graceful degradation** | Auto-fallback when hardware fails or is unavailable |

### Rust Nodes with Simulation

All HORUS hardware nodes use a **Backend enum pattern** with `Simulation` as an option:

```rust
use horus_library::nodes::imu::{ImuNode, ImuBackend};
use horus_library::nodes::gps::{GpsNode, GpsBackend};
use horus_library::nodes::lidar::{LidarNode, LidarBackend};

// Explicit simulation mode
let imu = ImuNode::new_with_backend("imu", ImuBackend::Simulation)?;
let gps = GpsNode::new_with_backend("gps.fix", GpsBackend::Simulation)?;
let lidar = LidarNode::new_with_backend("scan", LidarBackend::Simulation)?;

// Default constructors use simulation mode
let imu = ImuNode::new()?;  // Simulation by default
```

**Nodes with simulation support:**

| Node | Backend Enum | Hardware Backends |
|------|--------------|-------------------|
| `SerialNode` | `SerialBackend::Simulation` | `Hardware` |
| `ImuNode` | `ImuBackend::Simulation` | `Mpu6050`, `Bno055` |
| `GpsNode` | `GpsBackend::Simulation` | `NmeaSerial` |
| `LidarNode` | `LidarBackend::Simulation` | `RplidarA2`, `YdlidarX4` |
| `CameraNode` | (use sim2d/sim3d) | `OpenCV`, `V4L2`, `RealSense` |
| `DepthCameraNode` | `DepthBackend::Simulation` | `RealSense`, `Zed` |
| `DcMotorNode` | `MotorDriver::Simulation` | `L298N`, `Cytron` |
| `EncoderNode` | `EncoderBackend::Simulation` | `Gpio`, `Spi` |
| `DynamixelNode` | Auto-fallback | `Hardware` |
| `RoboclawMotorNode` | Auto-fallback | `Hardware` |
| `I2cBusNode` | Auto-fallback | `Hardware` |
| `SpiBusNode` | Auto-fallback | `Hardware` |

### Python Nodes with Simulation

Python hardware nodes follow the same pattern:

```python
from horus.nodes import (
    SerialNode, JoystickNode, KeyboardNode,
    ImuNode, GpsNode, CameraNode, LidarNode
)

# Explicit simulation mode
serial = SerialNode(port="/dev/ttyUSB0", simulation=True)
imu = ImuNode(i2c_bus=1, simulation=True)
gps = GpsNode(port="/dev/ttyACM0", simulation=True)
camera = CameraNode(device_id=0, simulation=True)
lidar = LidarNode(port="/dev/ttyUSB0", simulation=True)

# Auto-fallback: If hardware unavailable, switches to simulation
serial = SerialNode(port="/dev/ttyUSB0")  # Falls back if port doesn't exist
```

**Python nodes with simulation:**

| Node | Library | Simulation Data |
|------|---------|-----------------|
| `SerialNode` | pyserial | Accepts writes, no reads |
| `JoystickNode` | pygame | Zero axes/buttons |
| `KeyboardNode` | pynput | No events |
| `ImuNode` | smbus2 | Gravity (0,0,9.81), small gyro drift |
| `GpsNode` | pynmea2 | San Francisco coordinates |
| `CameraNode` | opencv | Black frame with "SIMULATION" text |
| `LidarNode` | rplidar | Circular room pattern |

### Auto-Fallback Behavior

Many nodes automatically fall back to simulation when hardware fails:

```rust
// Rust: Auto-fallback on hardware error
impl Node for ImuNode {
    fn init(&mut self, ctx: &mut NodeInfo) -> Result<()> {
        match self.backend {
            ImuBackend::Mpu6050 => {
                if let Err(e) = self.initialize_mpu6050() {
                    ctx.log_warning(&format!("MPU6050 init failed: {}, falling back to simulation", e));
                    self.backend = ImuBackend::Simulation;
                }
            }
            ImuBackend::Simulation => {
                ctx.log_info("IMU running in simulation mode");
            }
        }
        Ok(())
    }
}
```

```python
# Python: Auto-fallback on hardware error
def _init(self, node):
    if self.simulation:
        self.log_info("ImuNode running in simulation mode")
        return

    try:
        self._bus = smbus2.SMBus(self.i2c_bus)
        self._bus.write_byte_data(self.i2c_address, 0x6B, 0)  # Wake MPU6050
    except Exception as e:
        self.log_error(f"Failed to initialize IMU: {e}")
        self.simulation = True  # Auto-fallback
```

### Simulation Data Quality

Simulation mode generates **realistic synthetic data**:

**IMU Simulation:**
- Gravity vector: (0, 0, 9.81) m/s²
- Small gyroscope drift over time
- Temperature: 25°C constant

**GPS Simulation:**
- Default: San Francisco (37.7749, -122.4194)
- Configurable via `set_simulation_position()`
- Realistic satellite count and HDOP

**LiDAR Simulation:**
- Circular room pattern with walls
- Gaussian noise on range measurements
- Configurable num_samples, range_min, range_max

**Camera Simulation:**
- Black frame with "SIMULATION" overlay
- Frame counter displayed
- Configurable resolution

### Best Practices

**1. Develop with simulation first:**
```bash
# Start with simulation
cargo run --example my_robot  # Uses simulation by default

# Then test with hardware
cargo run --example my_robot --features hardware
```

**2. Use feature flags for hardware:**
```toml
# Cargo.toml
[features]
default = []  # Simulation by default
hardware = ["mpu6050", "rplidar", "gpio"]
```

**3. CI/CD with simulation:**
```yaml
# .github/workflows/test.yml
- name: Run tests
  run: cargo test  # No hardware needed
```

**4. Gradual hardware transition:**
```rust
// Start with all simulation
let imu = ImuNode::new()?;  // Simulation
let gps = GpsNode::new()?;  // Simulation

// Enable hardware one-by-one
let imu = ImuNode::new_with_backend("imu", ImuBackend::Mpu6050)?;
let gps = GpsNode::new()?;  // Still simulation
```

---

## Sim-to-Real Transfer

### Why HORUS Simulators Work for Real Robots

**1. Identical Communication:**
```rust
// Same code in simulation and reality
let cmd_pub = Hub::<CmdVel>::new("cmd_vel")?;
let scan_sub = Hub::<LaserScan>::new("scan")?;

// HORUS handles whether it's sim or real hardware
```

**2. Realistic Latency:**
- Sim IPC: 85-167ns (sim2d), ~300ns (sim3d)
- Real robot serial: 1-10ms
- Sim is actually *faster* than real hardware

**3. Same Message Types:**
```rust
// horus_library/messages - works everywhere
use horus::prelude::*; // Provides {CmdVel, LaserScan, Odometry};
```

**4. Physics Accuracy:**
- Rapier physics engine (production-grade)
- Realistic kinematics
- Accurate collision detection
- Configurable noise models

### Best Practices

**1. Start in Simulation:**
```bash
# Develop in sim2d (fast iteration)
horus sim
horus run navigation.rs

# Refine in sim3d (realistic testing)
horus sim3d --robot your_robot.urdf

# Deploy to real hardware
horus run --remote 192.168.1.100 navigation.rs
```

**2. Use Realistic Parameters:**
```yaml
# Match real robot specs
robot:
  max_linear_vel: 0.22   # Your robot's max speed
  max_angular_vel: 2.84
  wheel_radius: 0.033
  wheel_separation: 0.16
```

**3. Test with Noise:**
```yaml
sensors:
  lidar:
    noise_std: 0.01      # Realistic sensor noise
  odometry:
    drift_rate: 0.001    # Wheel slip
```

**4. Gradual Transfer:**
1. Perfect sim (no noise)  Algorithm works
2. Sim with noise  Algorithm robust
3. Real hardware  Fine-tune parameters

---

## Comparison with Other Simulators

| Feature | sim2d | sim3d (planned) | Gazebo | Webots | Isaac Sim |
|---------|-------|-----------------|--------|--------|-----------|
| **Dimensions** | 2D | 3D | 3D | 3D | 3D |
| **Physics** | Rapier2D | Rapier3D | ODE/Bullet | ODE | PhysX |
| **HORUS Native** | Yes | Yes | No | No | No |
| **IPC Latency** | 85-167ns | ~300ns | 10ms+ | 10ms+ | N/A |
| **Headless Speed** | 1000+ Hz | 100K+ steps/sec | 100 Hz | 100 Hz | 1000+ Hz |
| **RL Ready** | Basic | Yes | Plugins | Plugins | Yes |
| **URDF Support** | No | Yes | Yes | Yes | Yes |
| **Language** | Rust | Rust | C++ | C++ | Python |
| **License** | MIT | MIT | Apache | Apache | Proprietary |

**When to use each:**
- **sim2d**: Fast 2D navigation prototyping, CI/CD testing
- **sim3d**: Realistic 3D testing, RL training, complex robots
- **Gazebo**: ROS integration, existing ROS packages
- **Webots**: Education, established workflows
- **Isaac Sim**: NVIDIA GPU, photorealistic rendering, large-scale RL

---

## Getting Help

### Documentation
- sim2d: `/home/lord-patpak/horus/HORUS/horus_library/tools/sim2d/README.md`
- sim3d spec: `/home/lord-patpak/horus/HORUS/horus_library/tools/sim3d/SIM3D_SPEC.md`

### Community
- GitHub Discussions: https://github.com/softmata/horus/discussions
- Discord: Ask in #simulation channel
- Issues: Report bugs at https://github.com/softmata/horus/issues

### Examples

Find simulation examples in:
```bash
horus_library/apps/
├── snakesim/        # Snake game (sim2d demo)
├── tanksim/         # Tank simulation (sim2d)
└── sim3d_examples/  # sim3d examples (coming soon)
```

---

## Contributing

Help shape HORUS simulation tools:

**sim2d (Active Development):**
- Report bugs and issues
- Suggest features and improvements
- Improve documentation
- Add test scenarios

**sim3d (Upcoming):**
- Provide feedback on specification
- Suggest priority features
- Volunteer for implementation

See the [GitHub repository](https://github.com/softmata/horus) for contribution guidelines.

---

## Roadmap

### sim2d
- [DONE] Basic 2D physics and rendering
- [DONE] LiDAR sensor simulation
- [DONE] HORUS Hub integration
- [WIP] Multi-robot support (in progress)
- [WIP] Additional sensors (camera, IMU)
- [TODO] Scene editor
- [TODO] Scenario library

### sim3d
- [DONE] Complete technical specification
- [TODO] Core implementation (Q1-Q2 2026)
- [TODO] URDF support (Q2 2026)
- [TODO] 3D sensors (Q2 2026)
- [TODO] RL features (Q3 2026)
- [TODO] Beta release (Q4 2026)

---

## Next Steps

**Ready to simulate?**

1. **[Install HORUS](/getting-started/installation)** - Set up your environment
2. **[Quick Start](/getting-started/quick-start)** - Build your first robot
3. **[Try sim2d](https://github.com/softmata/horus/tree/main/horus_library/tools/sim2d)** - Start simulating
4. **[Join Discord](https://discord.gg/horus)** - Connect with the community

**Questions?**
- Check [Troubleshooting](/troubleshooting)
- Ask on [GitHub Discussions](https://github.com/softmata/horus/discussions)
